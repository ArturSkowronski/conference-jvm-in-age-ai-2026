============================================================
Llama3.java - Pure Java LLM Inference
============================================================
Java: java version "21.0.5" 2024-10-15 LTS
Model: /Users/askowronski/Priv/talks/conference-jvm-in-age-ai-2026/models/Llama-3.2-1B-Instruct-f16.gguf
Mode: --instruct
============================================================

WARNING: Using incubator modules: jdk.incubator.vector
Note: /Users/askowronski/Priv/talks/conference-jvm-in-age-ai-2026/demos/llama3-java/Llama3.java uses preview features of Java SE 21.
Note: Recompile with -Xlint:preview for details.
Parse /Users/askowronski/Priv/talks/conference-jvm-in-age-ai-2026/models/Llama-3.2-1B-Instruct-f16.gguf: 556 millis
Load LlaMa model: 245 millis
Here's one:

Why did the programmer quit his job?

Because he didn't get arrays.
context: 37/256 prompt: 12.73 tokens/s (17) generation: 13.20 tokens/s (20)
